{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract refusal\n",
    "\n",
    "This notebook shows how to run harmful and harmless instructions on an LLM and extract refusal embeddings\n",
    "\n",
    "- Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
    "- Adapted by [Fabian Hildebrandt](https://huggingface.co/FabianHildebrandt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "import gc\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters from config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print(config)\n",
    "\n",
    "MODEL_ID = config['extraction']['MODEL_ID']\n",
    "MODEL_TYPE = config['extraction']['MODEL_TYPE']\n",
    "MODEL_NAME = config['extraction']['MODEL_NAME']\n",
    "HARMFUL_DATA = config['extraction']['HARMFUL_DATA']\n",
    "HARMLESS_DATA = config['extraction']['HARMLESS_DATA']\n",
    "N_SAMPLES = config['extraction']['N_SAMPLES']\n",
    "TOKEN = config['extraction']['TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn automatic differentiation off to save GPU memory (credit: Undi95)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def reformat_texts(texts):\n",
    "    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n",
    "\n",
    "# Get harmful and harmless datasets\n",
    "def get_harmful_instructions():\n",
    "    dataset = load_dataset(HARMFUL_DATA)\n",
    "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    dataset = load_dataset(HARMLESS_DATA)\n",
    "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
    "\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "\n",
    "print('-----Overview Train & Test Data------')\n",
    "print(f\"Train data: Harmful instructions: {len(harmful_inst_train)}, Harmless instructions: {len(harmless_inst_train)}\")\n",
    "print(f\"Test data: Harmful instructions: {len(harmful_inst_test)}, Harmless instructions: {len(harmless_inst_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map='auto', # make sure, that model will be load to the GPU\n",
    "    token = TOKEN\n",
    ")\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16,\n",
    "    default_padding_side='left',\n",
    "    device_map='auto', # make sure, that model will be load to the GPU\n",
    "    use_auth_token = TOKEN\n",
    ")\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model):\n",
    "    \"\"\"Analyze a HookedTransformer model and return its key information\"\"\"\n",
    "    model_info = {\n",
    "        'Name': model.cfg.model_name,  # Use cfg instead of config\n",
    "        'Number of layers': model.cfg.n_layers,\n",
    "        'Attention heads / layer': model.cfg.n_heads,\n",
    "        'Hidden layer size': model.cfg.d_model,\n",
    "        'Number of different tokens': model.cfg.d_vocab,\n",
    "        'Context Size': model.cfg.n_ctx\n",
    "    }\n",
    "\n",
    "    # Calculate total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    model_info['total_parameters'] = total_params\n",
    "\n",
    "    return model_info\n",
    "\n",
    "# Example usage\n",
    "model_info = analyze_model(model)\n",
    "print('--------------')\n",
    "print('Model Information:')\n",
    "print('--------------')\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions(tokenizer, instructions):\n",
    "    #tokenizer.chat_template = \"<|im_start|>user\\n{message}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    if 'bloom' in MODEL_NAME:\n",
    "      tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "      {% if message['role'] == 'user' %}\n",
    "      User: {{ message['content'] }}\n",
    "      {% elif message['role'] == 'assistant' %}\n",
    "      Assistant: {{ message['content'] }}\n",
    "      {% endif %}\n",
    "      {% endfor %}\n",
    "      \"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        instructions,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        add_generation_prompt=True,\n",
    "    ).input_ids\n",
    "\n",
    "n_inst_train = min(N_SAMPLES, len(harmful_inst_train), len(harmless_inst_train))\n",
    "\n",
    "# Tokenize datasets\n",
    "harmful_tokens = tokenize_instructions(\n",
    "    tokenizer,\n",
    "    instructions=harmful_inst_train[:n_inst_train],\n",
    ")\n",
    "harmless_tokens = tokenize_instructions(\n",
    "    tokenizer,\n",
    "    instructions=harmless_inst_train[:n_inst_train],\n",
    ")\n",
    "print(f'Successfully tokenized {len(harmless_tokens)} harmless and {len(harmful_tokens)} harmful instructions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generations and cache the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size based on available VRAM\n",
    "batch_size = 16\n",
    "\n",
    "# Initialize defaultdicts to store activations\n",
    "harmful = defaultdict(list)\n",
    "harmless = defaultdict(list)\n",
    "\n",
    "# Process the training data in batches\n",
    "num_batches = (n_inst_train + batch_size - 1) // batch_size\n",
    "for i in tqdm(range(num_batches)):\n",
    "    print(i)\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min(n_inst_train, start_idx + batch_size)\n",
    "\n",
    "    # Run models on harmful and harmless prompts, cache activations\n",
    "    harmful_logits, harmful_cache = model.run_with_cache(\n",
    "        harmful_tokens[start_idx:end_idx],\n",
    "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
    "        device='cpu',\n",
    "        reset_hooks_end=True\n",
    "    )\n",
    "    harmless_logits, harmless_cache = model.run_with_cache(\n",
    "        harmless_tokens[start_idx:end_idx],\n",
    "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
    "        device='cpu',\n",
    "        reset_hooks_end=True\n",
    "    )\n",
    "\n",
    "    # Collect and store the activations\n",
    "    for key in harmful_cache:\n",
    "        harmful[key].append(harmful_cache[key][:,-1,:])\n",
    "        harmless[key].append(harmless_cache[key][:,-1,:])\n",
    "\n",
    "    # Flush RAM and VRAM\n",
    "    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful = {k: torch.cat(v) for k, v in harmful.items()}\n",
    "harmless = {k: torch.cat(v) for k, v in harmless.items()}\n",
    "print(f'Now we have the activations for all {model.cfg.n_layers} layers: {harmful.keys()}')\n",
    "print(f'Examplary shape of the harmful activations: {harmful[\"blocks.0.hook_resid_pre\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate difference-in-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get activation index\n",
    "def get_act_idx(cache_dict, act_name, layer):\n",
    "    key = (act_name, layer)\n",
    "    return cache_dict[utils.get_act_name(*key)]\n",
    "\n",
    "# Compute difference of means between harmful and harmless activations at intermediate layers\n",
    "activation_layers = [\"resid_mid\", \"resid_post\"]\n",
    "activation_refusals = defaultdict(list)\n",
    "\n",
    "for layer_num in range(1, model.cfg.n_layers):\n",
    "\n",
    "    for layer in activation_layers:\n",
    "        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, :].mean(dim=0)\n",
    "        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, :].mean(\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        refusal_dir = harmful_mean_act - harmless_mean_act\n",
    "        refusal_dir = refusal_dir / refusal_dir.norm()\n",
    "        activation_refusals[layer].append(refusal_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store activations in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = f\"./data\"\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "activations = {\n",
    "    'harmful': harmful,\n",
    "    'harmless': harmless,\n",
    "    'activation_refusals': activation_refusals\n",
    "}\n",
    "\n",
    "fpath = os.path.join(destination, f'{MODEL_NAME}_activations.pkl')\n",
    "with open(fpath, 'wb') as f:\n",
    "    pickle.dump(activations, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f'Successfully loaded to {fpath}.')\n",
    "\n",
    "# free up the ressources\n",
    "del harmful, harmless, activation_refusals\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
